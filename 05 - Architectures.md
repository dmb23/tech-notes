
# Transformer (General Pretrained Transformer)

- single Attention Head: transforms embedding of input vector to new embedding that attends to different parts of the sequence
- multiple Attention heads in parallel per Attention Block
- multiple layers of Attention Blocks after each other